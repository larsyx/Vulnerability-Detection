\chapter{Metodologia di analisi} \label{1cap:design}
% [titolo ridotto se non ci dovesse stare] {titolo completo}
%
\markboth{Metodologia di analisi}{}

\section{Obiettivi dello studio}
Questo studio si pone l'obiettivo di indagare su come e in che modo un predittore basato su machine learning può individuare la presenza di vulnerabilità software in un file sorgente, analizzando su come integrare il predittore durante la creazione del prodotto software, con lo scopo di facilitare l'individuazione di possibili criticità di sicurezza durante lo sviluppo del software.
Entrando più nel dettaglio lo studio si concentrerà sull'analisi di diverse metriche software, indagando come queste possono influire sulla presenza di possibili vulnerabilità software e quali sono le più caratterizzanti tra queste.
L'implementazione è stata guidata seguendo modelli di machine learning creati in studi precedenti fatti in altri contesti applicativi, che sono stati descritti nel capitolo~\ref{1cap:background}, cercando di adattare quanto più possibile il nuovo modello creato al mio contesto, in modo tale da migliorarne le prestazioni.

\section{Contesto dello studio}
L'obiettivo di questo studio è la creazione di un modello di machine learning per la predizione di vulnerabilità software in singoli file per applicazioni Java, questa scelta è stata guidata dall'importanza che il linguaggio Java ricopre nel campo informatico, infatti con il passare degli anni le applicazioni scritte in questo linguaggio sono sempre più numerose e complesse, ricoprendo un ruolo sempre più influente nella nostra società, per cui lo sviluppo di un modello di predizione in questo ambito aiuterebbe un gran numero di aziende e di sviluppatori, inoltre ho deciso di analizzare i singoli file in modo tale da avere una predizione che desse un'indicazione precisa su dove individuare la vulnerabilità presente nel prodotto software preso in considerazione.\newline 
Per il mio studio ho deciso considerare due progetti Java open-sorce, i progetti scelti sono Cloud Foundry \citep{cloudFoundry} e Spring Framework \citep{Spring-framework}.
Come mostrato dalla tabella~\ref{tab:Progetti} i due progetti hanno differenti grandezze e il progetto Spring Framework ha una grandezza e una complessità maggiore, in totale nei due progetti abbiamo \numprint{7723} file tra questi ho estratto \numprint{144} file vulnerabili.
Per l'estrazione dei file vulnerabili ho considerato il dataset \citep{ponta2019msr} creato nel 2019, il quale è stato creato con l'intenzione di collezionare per ogni progetto preso in considerazione tutte le vulnerabilità riscontrate durante lo sviluppo e la manutenzione dei seguenti prodotti software, tra questi sono stati analizzati anche i progetti presenti in questo studio.\newline
L'intera implementazione del modello è stata sviluppata e scritta in Python attraverso il tool online Google Colab, tutto il progetto compreso di dataset e Jupiter Notebook è presente su GitHub ed è accessibile attraverso il seguente link \url{https://github.com/larsyx/Vulnerability-Detection}. Per l'implementazione e l'analisi del modello di machine learning ho usato la libreria Python open-source \textit{Scikit-learn \citep{scikit-learn}}, che mi ha permesso di implementare in modo veloce e completo l'intero modello di predizione.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
        \rowcolor{lightgray}
         Progetto & Commits & File & File vulnerabili  \\
         Cloud Foundry & \numprint{9844} & 734 & 58\\
         Spring Framework & \numprint{24903} & \numprint{6989} & 86\\
     \hline
        \space & \textbf{\numprint{34747}} & \textbf{\numprint{7723}} & \textbf{144} \\
    \hline
    \end{tabular}
    \caption{Statistiche dei progetti presi in considerazione nello studio}
    \label{tab:Progetti}
\end{table}

\section{Estrazione dei dati}
In questa prima fase ho preparato e analizzato i dati che poi mi serviranno per il corretto funzionamento del modello di machine learning. 
La creazione del dataset è avvenuta attraverso il seguente dataset \citep{ponta2019msr}, da quest'ultimo ho estratto i file vulnerabili che sono stati risolti in passato per entrambi i progetti, ogni dato di questo dataset era composto da: progetto github, commit-ID in cui la vulnerabilità è stata risolta e il CVE-ID \citep{CVE} della vulnerabilità considerata, per l'estrazione dei file ho selezionato tutti i record presenti nel dataset per il progetti in questione e per ogni record ho estratto il file avente una vulnerabilità software attraverso il commit-ID. Dopo ciò ho ricavato un totale di 144 file vulnerabili dove 56 di questi nel progetto Cloud Foundry mentre 86 in Spring Framework. \newline
Il modello implementato dovrà predire la presenza di vulnerabilità software attraverso diverse metriche software, per cui successivamente ho deciso di porre la mia attenzione sulla selezione e l'individuazione di diverse metriche software che possono avere un impatto rilevante per la presenza di vulnerabilità in un file sorgente.  
 In una prima fase ho scelto le metriche software che andranno a caratterizzare il dataset creato, nello specifico ho deciso di prendere in considerazione 15 metriche software che sono riassunte nelle Tabelle~\ref{tab:metriche software} e~\ref{tab:metriche software2}, il motivo per cui ho deciso usare queste metriche è derivato dalla natura stessa del problema, infatti ho cercato di individuare tra le diverse metriche software, sulla base degli studi precedentemente fatti e analizzati nel capitolo \ref{1cap:background}, quelle più adatte al mio contesto.
 \newline
 Gran parte delle variabili indipendenti scelte mostrate nella tabella~\ref{tab:metriche software} sono le più classiche metriche software molte di queste estraibili facilmente dal codice sorgente, tra queste una delle metriche più interessanti per la valutazione di un file sorgente è la "\textit{Cyclomatic complexity}", quest'ultima è stata sviluppata con l'intenzione di stimare attraverso un numero il livello di complessità di un file, l'estrazione della complessità ciclomatica avviene individuando il numero di cammini linearmente indipendenti presenti in un file il risultato della somma di quest'ultimi indica il livello di complessità del file sorgente.\newline

\begin{table}[h]
    \centering
    \begin{tabular}{|c|p{11cm}|}
    \rowcolor{black}
         \textcolor{white}{\textbf{NOME}} & \textcolor{white}{\textbf{DESCRIZIONE}}  \\
         
         \textbf{HalsteadVocabulary} & Il totale tra tutti gli operatori unici e il numero di operandi unici.\\
         
         \rowcolor{lightgray}
         \textbf{HalsteadLenght} & Il totale tra il numero di occorrenze degli operandi e il numero di occorrenze degli operatori.\\
        
         \textbf{HalsteadVolume} & Rappresenta le dimensioni dello spazio necessario per ordinare un programma, il volume è proporzionale alla lunghezza del programma.\\
         \hline
    \end{tabular}
    \caption{Metriche di Halstead}
    \label{tab:metriche software2}
\end{table}

Tra le più interessanti metriche software scelte troviamo quelle create da Maurice Howard Halstead descritte nella tabella~\ref{tab:metriche software2}, queste metriche sono state sviluppate con lo scopo di misurare la complessità di un programma software senza la sua esecuzione, infatti queste sono state create con l'idea che le metriche software dovrebbero riflettere l'implementazione di algoritmi che compongono il programma. L'idea di base per l'estrazione delle metriche di Halstead sta nella suddivisione del codice sorgente in una sequenza di token, i token successivamente sono classificati e contati come \textit{operatori} e \textit{operandi}.\newline
 \begin{table}[]
     \centering
     \begin{tabular}{|c|p{3cm}|p{8cm}|}
     \hline
        \rowcolor{black}
         \textcolor{white}{\textbf{NOME}} & \textcolor{white}{\textbf{NOME ESTESO}} & \textcolor{white}{\textbf{DESCRIZIONE}}  \\
         
         \textbf{LOC} & Lines of Code & Conteggio numero di linee di codice e di commenti. \\
        
         \rowcolor{lightgray}
         \textbf{LCOM} & Lack of Cohesion of Methods & Descrive la mancanza di coesione tra i metodi della classe attraverso un numero, la numerazione avviene in modo crescente quindi un numero molto basso indica che la classe ha un livello di coesione molto alto, viceversa per un numero molto alto. \\
         
         \textbf{CBO} & Coupling \newline between Objects & Indica il numero di classi che sono legate a una particolare classe.\\
        
        \rowcolor{lightgray}
         \textbf{ELOC} & Effective Lines of Code & Indica il numero effettivo di linee di codice togliendo commenti e spazi vuoti.\\
         
         \textbf{McCabeMetric} & Cyclomatic complexity & Questa metrica stima il livello di complessità di una classe, la stima avviene analizzando cammini linearmente indipendenti di un file sorgente.\\
         
         \rowcolor{lightgray}
         \textbf{RFC} & Response For Class & Indica il numero di metodi che possono essere eseguiti quando un oggetto di quella classe riceve un messaggio.\\
        
         \textbf{MPC} & Message Passing Coupling & Indica il numero di messaggi che passano tra gli oggetti di una classe. Un valore alto indica un forte accoppiamento tra più classi.\\
 
         \rowcolor{lightgray}        
         \textbf{DAC} & Data Abstracting Coupling & Indica il numero di attributi in una classe che appartengono ad altre classi.\\
         
         \textbf{DAC2} & Data Abstracting Coupling 2 & Indica il numero di classi di cui gli attributi di una classe appartengono.\\

        \rowcolor{lightgray}         
         \textbf{WMC} & Weighted Methods for Class & Questa metrica indica la somma della complessità di tutti i metodi di una classe.\\
        
         \textbf{NOA} & Number of attributes & Indica il numero di attributi di una classe.\\

        \rowcolor{lightgray}         
         \textbf{NOPA} & Number of public attributes & Indica il numero di attributi pubblici presenti in una classe.\\
        \hline
     \end{tabular}
     \caption{Metriche software}
     \label{tab:metriche software}
 \end{table}
L'estrazione delle metriche software per la creazione del dataset è avvenuto attraverso il software "\textit{SwAnalytics}" creato dal SeSa Lab \citep{SeSa-Lab}, quest'ultimo mi ha permesso l'estrazione in formato csv di un intero progetto software fruibile su GitHub, il seguente programma mi ha dato anche la possibilità di scegliere il commit-ID del progetto di cui fare l'estrazione delle metriche. Per cui ho preso in considerazione l'ultimo commit effettuato dai progetti software di cui ho preso in considerazione, ho estratto i file contenti i risultati ed ho considerato quest'ultimi come file non vulnerabili. Successivamente ad ogni commit estratto precedentemente dal dataset \citep{ponta2019msr}, il quale mi dava conoscenza delle vulnerabilità presenti all'interno del progetto, ho estratto le metriche attraverso il commit-ID e dal file risultante ho estratto i file vulnerabili, che sono stati inseriti nel nuovo dataset costruito.   

\section{Analisi dei dati}
In questa fase del mio studio il lavoro eseguito è stato svolto per la preparazione dei dati e per l'implementazione e validazione del modello di machine learning. 
Prima di iniziare con queste fasi ho analizzato il dataset e il problema in questione e ho notato che il dataset usato per l'implementazione del modello di predizione contiene dati strutturati. \newline
Prima di procedere con la fase di preparazione dei dati ho suddiviso il dataset iniziale in \textit{training set} e \textit{test set}. 

\subsection{Preparazione dei dati}
Durante questa fase il mio obiettivo è stato di preparare i dati per la successiva fase di addestramento, infatti una maggiore qualità dei dati in fase di addestramento contribuisce in modo significativo alla costruzione di un modello di machine learning che operi in modo corretto e con un alto livello di precisione, questa è una delle fasi più cruciali per ottenere un modello di predizione efficiente e che operi in modo corretto successivamente nella fase di rilascio. Alcune delle seguenti operazioni sono state effettuate esclusivamente sul training set in modo tale da evitare fenomeni di \textit{data leakage}, questo problema è causato da diversi fattori, la causa principale è l'uso durante il processo di addestramento di informazioni che non dovrebbero essere disponibili al momento della predizione \citep{kaufman2012leakage}, un modello di predizione affetto da data leakage rende il modello capace di lavorare in modo accurato in fase di addestramento ma non in fase di rilascio.


\subsubsection{Pulizia dei dati}
La pulizia dei dati è la prima fase per la preparazione dei dati e ha l'obiettivo di rimuovere dal dataset dati superflui che non saranno utili per la predizione, e di rimuovere dati mancanti o nulli, in modo tale da dare al modello di predizione dati completi e non generino problemi di data leakage.\newline Considerando il dataset costruito precedentemente ho deciso di rimuovere le colonne che indicano il commit-ID del progetto e il nome della classe che presentava una vulnerabilità, il motivo è legato alla natura dei dati infatti quest'ultimi non hanno nessuna correlazione con la presenza di vulnerabilità e potrebbero compromettere il corretto funzionamento del modello di predizione, inoltre durante la predizione, dopo il suo rilascio, il modello non avrà a disposizione questi dati. Successivamente ho analizzato il dataset e ho riscontrato che il dataset non presenta di dati mancanti o nulli.

\subsubsection{Normalizzazione delle feature}
Lo scopo di questa fase è quello di normalizzare tutte le variabili indipendenti del nostro dataset in modo tale che il modello di machine learning predirrà attraverso l'uso di variabili che hanno la stessa scala di valori, la normalizzazione permette di evitare che ordini di grandezza diversi vadano ad avere un impatto sul modello. \newline
Analizzando il mio dataset ho notato che tutte le metriche software presenti nel mio dataset sono variabili numeriche e non categoriche e con scale molto diverse tra loro, quindi ho deciso di normalizzarle tutte, con un scala che va da 0 a 100 per tutte le variabili, a seguito di alcune analisi ho deciso che il processo di normalizzazione dei dati avvenga attraverso la \textit{min-max normalization}, questa tecnica permette di normalizzare i dati in modo veloce ed efficiente a partire dai valori di massimo e di minimo registrati in una determinata caratteristica.

\subsubsection{Selezione delle feature}
Questa è una delle fasi più importanti dell'intero processo che caratterizzerà fortemente le prestazioni del modello di machine learning, nello specifico durante questa fase si dovranno definire quali e quante variabili indipendenti (feature) saranno utilizzate dal modello di predizione durante la sua esecuzione. \newline
Il modo attraverso cui ho deciso di effettuare la selezione delle variabili è la \textit{feature extraction} ovvero attraverso la generazione automatica delle feature, che avviene attraverso l'uso di diversi algoritmi, Scikit-learn \citep{scikit-learn} mette a disposizione diversi algoritmi per la feature extraction, in questo studio ho deciso di utilizzare la funzione \textit{SelectKBest} che mi ha permesso di selezionare le migliori feature del dataset. La decisione sul numero di feature che il modello dovrà utilizzare per la predizione è stato scelto empiricamente effettuando diversi test in fase di modellazione e validazione, alla fine di questo processo ho deciso di selezionare 7 feature, le feature scelte nella versione finale sono: \textit{LCOM}, \textit{CBO}, \textit{McCabeMetric}, \textit{WMC}, \textit{HalsteadVocabulary}, \textit{HalsteadLenght} e \textit{HalsteadVolume}.


\subsubsection{Bilanciamento dei dati}
Questa è l'ultima fase per la preparazione dei dati, in quest'ultima parte si decide se e in che modo bilanciare il dataset, esistono due strategie di bilanciamento \textit{undersampling} che consiste nel eliminare casualmente istanze della classe di maggioranza e \textit{oversampling} che consiste nella generazione di istanze delle classe di minoranza.\newline
Analizzando il mio dataset e da come si evince dalla tabella \ref{tab:Progetti} il dataset presenta un forte sbilanciamento verso le classi non vulnerabili, inoltre nel mio dataset ho riscontrato che non ho a disposizione un elevato numero di istanze, per cui la rimozione di ulteriori istanze porterebbe il modello a lavorare su un numero molto ridotto di dati, sulla base di queste osservazioni ho deciso di generare nel mio dataset istanze di classi vulnerabili in modo tale da bilanciarlo, per la generazione delle istanze ho deciso di usare la funzione \textit{SMOTE} presente nella libreria di Scikit-learn.


\subsection{Modellazione dei dati}
Lo scopo principale di questa fase consiste nella definizione del problema di predizione e nella scelta degli algoritmi che saranno utilizzati, successivamente si passa alla fase di addestramento e nella scelta dei parametri degli algoritmi scelti in modo tale da migliorare le prestazioni del modello di predizione. \newline
Analizzando l'obiettivo del mio modello di predizione ho compreso che il problema è un classificabile come un problema di classificazione, in quanto il predittore dovrà stabilire se un'istanza di una classe è classificabile come file vulnerabile o non vulnerabile. Per l'implementazione ho deciso di scegliere diversi classificatori tra i più comuni, che sono elencati e descritti nella tabella \ref{tab:Classificatori}, lo scopo è di verificare empiricamente le loro prestazioni e valutare quale classificatore ottiene una maggiore precisione in questo ambito.
Successivamente in base ai risultati ottenuti da ogni classificatore singolarmente ho definito i parametri per la creazione dei classificatori.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|p{10cm}|}
         \rowcolor{black}
         \textcolor{white}{\textbf{NOME}} & \textcolor{white}{\textbf{DESCRIZIONE}}  \\
         \textbf{Naive Bayes} & Questo classificatore fa parte della famiglia dei classificatori probabilistici, e come tale basa la sua predizione sulla applicazione del teorema di Bayes. \\
         
         \rowcolor{lightgray}
         \textbf{Decision Tree} & Questo classificatore crea, attraverso il valore di \textit{information gain} per ogni feature, un albero decisionale dal quale poi baserà la sua predizione. \\
         
         \textbf{Random Forest} & Questo classificatore è un metodo di classificazione \textit{ensemble}, il suo funzionamento consiste nella creazione di diversi alberi decisionali da cui estrae la sua predizione. \\
         
         \rowcolor{lightgray}
         \textbf{Support Vector Machine} & Questo classificatore utilizza i vettori di supporto per la creazione di uno o più iperpiani che dividono lo spazio di più dimensioni, il quale sarà utilizzato per la predizione.\\
         
         \textbf{AdaBoost} & È uno dei classificatori \textit{ensemble}, il suo funzionamento prevede la costruzione di una lista di classificatori aventi ognuno un peso in base alle sue prestazioni, infine la predizione avviene considerando tutti i classificatori in base al loro peso.\\
         \hline
    \end{tabular}
    \caption{Classificatori utilizzati per la creazione del modello di predizione}
    \label{tab:Classificatori}
\end{table}


\subsection{Validazione del modello}
% Descrizione validazione
Questa è l'ultima fase del intero processo di creazione del modello di machine learning, l'obiettivo consiste nella verifica del corretto funzionamento del modello di predizione creato, analizzando e verificando che rispetti tutti gli obiettivi posti precedentemente, nel caso che questi non sono rispettati si torna alla fase di modellazione per correggere e migliorare il modello, in questa fase inoltre vengono analizzati, attraverso l'uso di diverse metriche, le prestazioni raggiunte dal predittore.\newline
% Strategia di validazione recap dei risultati
La strategia di validazione scelta prevedeva la divisione del dataset in training set e test set con un rapporto rispettivo di 67\% e 33\%, il training set è stato usato esclusivamente per l'addestramento del modello, mentre il test set è stato usato esclusivamente per la validazione del modello, il motivo legato a questa suddivisione sta nella natura del dataset, siccome il mio dataset di partenza non ha un numero di istanze particolarmente corposo, questa proporzione mi permette di dare in input alla fase di addestramento un numero adeguato di istanze.
Da questa fase ho appreso come i diversi classificatori si comportano e quali di questi ha registrato prestazioni più precise e accurate, nello specifico i classificatori che ha registrato indici prestazionali più interessanti sono il \textit{Decision Tree} e il \textit{Random Forest} mentre i risultati più bassi sono stati registrati dal \textit{Naive Bayes}.
% metriche di valutazione
\newline
La scelta delle metriche di valutazione è avvenuta valutando, tra quelle esistenti, quelle più utili e complete che mi permettono di avere una visione completa e precisa delle prestazioni raggiunte dai diversi modelli implementati, per cui tra le diverse metriche di valutazione ho deciso di scegliere per la validazione le seguenti metriche: 
\begin{itemize}
    \item \textit{recall}: questa metrica indica quante istanze positive nell'intero dataset il classificatore può determinare, la sua formula matematica è \(\frac{veropositivo}{veropositivo + falsopositivo} \);
    \item \textit{precision}: questa metrica indica quanti errori ci saranno nella lista delle predizioni fatte dal classificatore, la sua formula matematica è \(\frac{veropositivo}{veropositivo + falsonegativo} \);
    \item \textit{accuratezza}: questa metrica indica il numero totale di predizioni corrette, la sua formula matematica è \(\frac{veropositivo + veronegativo}{veropositivo +veronegativo + falsopositivo + falsonegativo} \);
    \item \textit{f-measure}: questa metrica è ottenuta attraverso la media armonica tra precision e recall, ed è una misura che indica il livello di accuratezza del classificatore.
\end{itemize}   
Le prestazioni ottenute dai diversi classificatori sono descritte e analizzate nel capitolo~\ref{1cap:analisi}.
\newpage
